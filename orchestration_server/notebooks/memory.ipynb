{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def extract_data():\n",
    "    data_extractor = DataExtractor(remote_url=\"http://localhost:4444\")\n",
    "    data_extractor.fill_search_bar_by_link(job=\"data scientist\", location=\"France\", from_last_days=3)\n",
    "    data_extractor.driver.find_element(By.ID, \"onetrust-reject-all-handler\").click()\n",
    "    data = data_extractor.get_job_descriptions()\n",
    "    data_extractor.store_data_in_mongo(data)\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'you',\n",
    "    'start_date': datetime.now(),\n",
    "    'depends_on_past': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='data_extractor_dag', default_args=default_args, schedule_interval=timedelta(hours=24))\n",
    "\n",
    "extract_data_task = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable=extract_data,\n",
    "    dag=dag)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, that's correct. In this example, there is only one task defined in the DAG, the extract_data_task . The task is defined by the PythonOperator and it calls the extract_data() function when it runs.\n",
    "The extract_data() function in turn calls the different methods of your class to perform the data extraction, cleaning and storing steps.\n",
    "\n",
    "You can add more tasks to the DAG if your pipeline includes more steps. For example, you could add a task to perform data analysis and another task to update the dashboard with the results.\n",
    "\n",
    "You can also use other type of operators like BashOperator or PythonOperator to perform other tasks.\n",
    "You can also use subdag or shortcircuit operator to create a more complex pipeline with multiple tasks.\n",
    "\n",
    "Keep in mind that each task should be atomic and idempotent, and should only perform one specific action. This makes it easier to troubleshoot and maintain your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def extract_data():\n",
    "    data_extractor = DataExtractor(remote_url=\"http://localhost:4444\")\n",
    "    data_extractor.fill_search_bar_by_link(job=\"data scientist\", location=\"France\", from_last_days=3)\n",
    "    data_extractor.driver.find_element(By.ID, \"onetrust-reject-all-handler\").click()\n",
    "    data = data_extractor.get_job_descriptions()\n",
    "    return data\n",
    "\n",
    "def store_data(**kwargs):\n",
    "    data = kwargs['ti'].xcom_pull(task_ids='extract_data')\n",
    "    data_extractor.store_data_in_mongo(data)\n",
    "\n",
    "def analyze_data(**kwargs):\n",
    "    data = kwargs['ti'].xcom_pull(task_ids='store_data')\n",
    "    # perform analysis\n",
    "    return analysis_result\n",
    "\n",
    "def update_dashboard(**kwargs):\n",
    "    analysis_result = kwargs['ti'].xcom_pull(task_ids='analyze_data')\n",
    "    #update dashboard\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'you',\n",
    "    'start_date': datetime.now(),\n",
    "    'depends_on_past': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='data_extractor_dag', default_args=default_args, schedule_interval=timedelta(hours=24))\n",
    "\n",
    "extract_data_task = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable=extract_data,\n",
    "    provide_context=True,\n",
    "    dag=dag)\n",
    "\n",
    "store_data_task = PythonOperator(\n",
    "    task_id='store_data',\n",
    "    python_callable=store_data,\n",
    "    provide_context=True,\n",
    "    dag=dag)\n",
    "\n",
    "analyze_data_task = PythonOperator(\n",
    "    task_id='analyze_data',\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first function extract_data(), I returned the extracted data instead of pushing it to XCOM (Airflow's task instance metadata store).\n",
    "When a task returns a value, it is implicitly passed to the next task in the DAG as an upstream_task_result parameter.\n",
    "\n",
    "For example, the store_data() function can access the data returned by the extract_data() function by calling kwargs['ti'].upstream_task_result which will have the return value of the extract_data() function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets, it is generally better to read and write the data from a remote storage, rather than passing the data between tasks.\n",
    "\n",
    "Passing large datasets between tasks in memory could cause performance issues and may even cause the pipeline to fail due to memory constraints. It also could cause issues with the airflow scheduler and worker, which could lead to unexpected behavior.\n",
    "\n",
    "Instead, you can store the data in a remote storage such as an object storage or a database and read and write the data from there. This way, you can avoid the need to pass large datasets between tasks and the data can be accessed by different tasks and even different DAGs.\n",
    "\n",
    "For example, you can use a remote storage such as S3 or GCS to store the scraped data and then use the Airflow's S3KeySensor or GoogleCloudStorageObjectSensor to trigger the next task when the data is available in the storage.\n",
    "\n",
    "You can also use a database like MongoDB or PostgreSQL to store the scraped data, and then use the MongoHook or PostgresHook to read and write the data from the database.\n",
    "\n",
    "By using a remote storage, you also gain the benefits of data persistence, data backup and versioning.\n",
    "\n",
    "It's also worth noting that depending on your use case, it might be more efficient to use a distributed data processing framework like Apache Hadoop, Apache Spark, or Apache Flink to process large datasets, rather than using Airflow alone."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to avoid storing the data in your local computer before uploading it, you can use a file-like object to store the data in memory and then write it to Minio. Here's an example of how you can use the io.BytesIO class to write data to Minio without storing it on your local computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.common.by import By\n",
    "from helpers import DataExtractor\n",
    "import pickle\n",
    "\n",
    "# Task to extract the data\n",
    "def extract_data(**kwargs):\n",
    "    data_extractor = DataExtractor(remote_url=\"http://20.224.70.229:4444\")\n",
    "    data_extractor.fill_search_bar_by_link(job=\"data scientist\", location=\"France\", from_last_days=3)\n",
    "    data_extractor.remove_pop_up()\n",
    "    data = data_extractor.get_job_descriptions()\n",
    "\n",
    "    # store the data into xcom to be used by the next task\n",
    "    kwargs['ti'].xcom_push(key='scraped_data', value=data)\n",
    "    data_extractor.close()\n",
    "\n",
    "\n",
    "# Task to load the data\n",
    "def load_data(**kwargs):\n",
    "    # Retrieve the data from the XCom\n",
    "    data = kwargs['ti'].xcom_pull(key='scraped_data', task_ids='extract_data')\n",
    "    print(\"this is the data: \", data)\n",
    "    \n",
    "    # Load the data into MongoDB\n",
    "    data_extractor = DataExtractor(remote_url=\"http://20.224.70.229:4444\")\n",
    "    data_extractor.store_data_in_mongo(data, bulk=False)\n",
    "    \n",
    "    # Close the connection\n",
    "    data_extractor.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define the default arguments for the DAG\n",
    "default_args = {\n",
    "    \"owner\": \"abdessamad\",  # the owner of the DAG\n",
    "    \"start_date\": datetime.now(),  # the start date of the DAG\n",
    "    \"depends_on_past\": True,  # the DAG depends on the past\n",
    "    \"retries\": 2,  # the number of retries\n",
    "    \"retry_delay\": timedelta(hours=1),  # the delay between retries\n",
    "    \"catchup\": False,  # the DAG does not catch up with the past\n",
    "    'schedule_interval': '@hourly',  # the schedule interval of the DAG\n",
    "}\n",
    "\n",
    "# Instantiate the DAG\n",
    "dag = DAG('data_extraction_and_loading_17', \n",
    "          default_args=default_args, \n",
    "          schedule_interval=None)\n",
    "\n",
    "# Instantiate the extract task\n",
    "extract_data_task = PythonOperator(task_id='extract_data', \n",
    "                                   python_callable=extract_data, \n",
    "                                   provide_context=True,\n",
    "                                   dag=dag)\n",
    "\n",
    "# Instantiate the load task\n",
    "load_data_task = PythonOperator(task_id='load_data', \n",
    "                                python_callable=load_data, \n",
    "                                provide_context=True, \n",
    "                                trigger_rule=\"all_success\",\n",
    "                                dag=dag)\n",
    "\n",
    "# Set the task dependencies\n",
    "extract_data_task >> load_data_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (v3.10.6:9c7b4bd164, Aug  1 2022, 17:13:48) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
